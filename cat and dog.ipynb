{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff29bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing keras libraries and packages\n",
    "# takes 2D image and flatten the image\n",
    "#actual input is a single array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "672591cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising the CNN. \n",
    "# Note: CNN is most commonly applied to analyze visual imagery\n",
    "\n",
    "classifier=Sequential()\n",
    "\n",
    "#classifies is a model. Classifier bcoz we are classifying dog or ccat\n",
    "\n",
    "#Step 1 - Convolution\n",
    "\n",
    "# using relu activation function\n",
    "\n",
    "#input layer\n",
    "classifier.add(Conv2D(32,(3,3), input_shape=(64,64,3),activation='relu'))#adding first layer\n",
    "\n",
    "# input_shape= picture coming in 64*64 pixel. each pixel has 3 values\n",
    "#Conv2D it to 2D setup\n",
    "#picture must formatted in the same size as given above\n",
    "\n",
    "#Step 2 - Adding second convolution layer\n",
    "\n",
    "#hidden layer\n",
    "classifier.add(Conv2D(32,(3,3),activation='relu'))\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#resize by 2*2\n",
    "# MaxPool2D: when we get to the end of the layer we map the data to all the layers and reduce it to two sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12944a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final setup which is to flatten\n",
    "\n",
    "# Step 3 - Flattening the image\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full Connection\n",
    "\n",
    "classifier.add(Dense(units=128,activation='relu'))#64+64=128 not we can play with other values also\n",
    "# Dense we are taking whatever it's coming\n",
    "\n",
    "classifier.add(Dense(units=1,activation='sigmoid'))#sigmoid make it's clear yes or no\n",
    "# we filter all and smash in single output i.e., units=1 bcoz we want answer whether it's dog or cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b08286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the CNN. CNN= classifieer neural network,adam is best for large datsets\n",
    "\n",
    "\n",
    "#optimizer= reverse propagation, when training it goes all the way through and says error and the how it's adjusted it's weight\n",
    "# there are many optimizer\n",
    "# loss and metrices computes the error\n",
    "\n",
    "# error can compute using Standard deviation, Standard deviation square or binary_crossentropy.\n",
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ba6d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True)\n",
    "# 255 = scale in color of pixel  i.e., 0 to 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27272e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                target_size=(64,64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "# batch size= how many images we are goona batch each time during training\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dba4d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# fromatting test set\n",
    "# any changes in training set will also made in test_set\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_set=train_datagen.flow_from_directory('dataset/test_set',\n",
    "                                          target_size=(64,64),\n",
    "                                          batch_size=32,\n",
    "                                          class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aea0846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 121s 479ms/step - loss: 0.6917 - accuracy: 0.5890 - val_loss: 0.6387 - val_accuracy: 0.6568\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 57s 226ms/step - loss: 0.6247 - accuracy: 0.6575 - val_loss: 0.5994 - val_accuracy: 0.6734\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 57s 227ms/step - loss: 0.5753 - accuracy: 0.6999 - val_loss: 0.5694 - val_accuracy: 0.7046\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 57s 227ms/step - loss: 0.5416 - accuracy: 0.7286 - val_loss: 0.5615 - val_accuracy: 0.7218\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.5190 - accuracy: 0.7466 - val_loss: 0.5501 - val_accuracy: 0.7177\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 62s 247ms/step - loss: 0.4999 - accuracy: 0.7565 - val_loss: 0.5271 - val_accuracy: 0.7490\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.4713 - accuracy: 0.7699 - val_loss: 0.5073 - val_accuracy: 0.7500\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.4549 - accuracy: 0.7790 - val_loss: 0.5021 - val_accuracy: 0.7621\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.4256 - accuracy: 0.7984 - val_loss: 0.5150 - val_accuracy: 0.7606\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.4133 - accuracy: 0.8091 - val_loss: 0.5453 - val_accuracy: 0.7409\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3935 - accuracy: 0.8221 - val_loss: 0.5175 - val_accuracy: 0.7666\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.3783 - accuracy: 0.8267 - val_loss: 0.5075 - val_accuracy: 0.7661\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.3512 - accuracy: 0.8440 - val_loss: 0.5342 - val_accuracy: 0.7616\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 0.3395 - accuracy: 0.8558 - val_loss: 0.5131 - val_accuracy: 0.7712\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 55s 221ms/step - loss: 0.3238 - accuracy: 0.8599 - val_loss: 0.5276 - val_accuracy: 0.7676\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 56s 225ms/step - loss: 0.2935 - accuracy: 0.8774 - val_loss: 0.5487 - val_accuracy: 0.7722\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.2876 - accuracy: 0.8774 - val_loss: 0.5643 - val_accuracy: 0.7571\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 58s 234ms/step - loss: 0.2661 - accuracy: 0.8896 - val_loss: 0.5618 - val_accuracy: 0.7601\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 63s 252ms/step - loss: 0.2561 - accuracy: 0.8980 - val_loss: 0.5772 - val_accuracy: 0.7686\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.2361 - accuracy: 0.9043 - val_loss: 0.5663 - val_accuracy: 0.7702\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.2061 - accuracy: 0.9206 - val_loss: 0.6525 - val_accuracy: 0.7681\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 60s 238ms/step - loss: 0.2159 - accuracy: 0.9139 - val_loss: 0.6182 - val_accuracy: 0.7686\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 60s 239ms/step - loss: 0.1898 - accuracy: 0.9229 - val_loss: 0.6408 - val_accuracy: 0.7686\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.1858 - accuracy: 0.9260 - val_loss: 0.6657 - val_accuracy: 0.7732\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.1785 - accuracy: 0.9321 - val_loss: 0.6290 - val_accuracy: 0.7812\n"
     ]
    }
   ],
   "source": [
    "#  Training the model\n",
    "\n",
    "batch_size=32\n",
    "history=classifier.fit(training_set,\n",
    "               steps_per_epoch=int(8000/batch_size),\n",
    "               epochs=25,\n",
    "               validation_data=test_set,\n",
    "               validation_steps=int(2000/batch_size)\n",
    "              )\n",
    "\n",
    "#  steps_per_epoch= no of photos\n",
    "# epoch= how many times we want to go through a dataset.\n",
    "# fit_generator =back propogation\n",
    "# information goes through forwaard with a picture and it says you're right or wrong and the it goes backward and reprogram all those weghts\n",
    "\n",
    "\n",
    "# acc and val_acc -  reflects bias while training the data\n",
    "# value accuracy run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f22ebaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "# part 3 - Making new predictions\n",
    "\n",
    "import numpy as np\n",
    "# from keras.preprocessing import image\n",
    "from keras.utils import load_img, img_to_array\n",
    "test_image=load_img('single_prediction/dog.webp',target_size=(64,64))#loading image\n",
    "\n",
    "#preprocessing image\n",
    "test_image=img_to_array(test_image)#setting image to an array\n",
    "test_image=np.expand_dims(test_image, axis=0)#expand dimension axis =0\n",
    "# running result\n",
    "result=classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0]==1:\n",
    "    prediction='dog'\n",
    "else:\n",
    "    prediction='cat'\n",
    "print(prediction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54eeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d03970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
